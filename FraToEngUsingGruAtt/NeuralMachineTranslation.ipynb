{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EZg7sHZh7fAD"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile(\"/content/data (1).zip\", 'r') as zObject:\n",
        "    zObject.extractall(path=\"/content/\")"
      ],
      "metadata": {
        "id": "MwcEkPpA7j2o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
        "    self.n_words = 2\n",
        "  def addSentence(self, sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        "  def addWord(self, word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else:\n",
        "      self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "A2dHKMKm8AMS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unicodeToAscii(s):\n",
        "  return ''.join(\n",
        "      c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn'\n",
        "  )\n",
        "\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "  s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "  return s.strip()"
      ],
      "metadata": {
        "id": "v-TYWyLI9vUj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "  print(\"Reading Lines......\")\n",
        "\n",
        "  lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
        "\n",
        "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "  if reverse:\n",
        "    pairs = [list(reversed(p)) for p in pairs]\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "  else:\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "  return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "Zk9gnnox-LzA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "  return len(p[0].split(' ')) < MAX_LEN and len(p[1].split(' ')) < MAX_LEN #and p[1].startswith(eng_prefixes)\n",
        "\n",
        "def filterPairs(pairs):\n",
        "  return [p for p in pairs if filterPair(p)]"
      ],
      "metadata": {
        "id": "FUf5p6Dh_xPR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse = False):\n",
        "  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "  print(\"Read %s sentence pairs\" %len(pairs))\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "  print(\"Counting words....\")\n",
        "  for pair in pairs:\n",
        "    input_lang.addSentence(pair[0])\n",
        "    output_lang.addSentence(pair[1])\n",
        "  print(\"Counted Words\")\n",
        "  print(input_lang.name, input_lang.n_words)\n",
        "  print(output_lang.name, output_lang.n_words)\n",
        "  return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng','fra',True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8bJ-FR9G5VA",
        "outputId": "131adccc-59a7-476d-c975-077dceba454b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Lines......\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 105692 sentence pairs\n",
            "Counting words....\n",
            "Counted Words\n",
            "fra 17864\n",
            "eng 10698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOw1kLlcH462",
        "outputId": "22ee669e-025f-4a2f-dd4d-5a3552674a48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['si je le savais je vous le dirais', 'if i knew that i d tell you']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, input):\n",
        "    embedded = self.dropout(self.embedding(input))\n",
        "    # print('embedded :', embedded.shape)\n",
        "    output, hidden = self.gru(embedded)\n",
        "    # print('output :', output.shape)\n",
        "    # print('Hidden :', hidden[0].shape)\n",
        "    return output, hidden"
      ],
      "metadata": {
        "id": "XXWnxwLRIAqA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(BahdanauAttention,self).__init__()\n",
        "    self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "    self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "    self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "  def forward(self, query, keys):\n",
        "    # print(self.Wa(query).shape)\n",
        "    # print(self.Ua(keys).shape)\n",
        "    scores = self.Va(torch.tanh( self.Wa(query) + self.Ua(keys) ))\n",
        "    # print('Scores :', scores.shape)\n",
        "    scores = scores.squeeze(2).unsqueeze(1)\n",
        "    # print('Scores :', scores.shape)\n",
        "\n",
        "    weights = F.softmax(scores, dim = -1)\n",
        "    # print('Weights : ', weights.shape)\n",
        "    context = torch.bmm(weights, keys)\n",
        "    # print('context : ', context.shape)\n",
        "    return context, weights"
      ],
      "metadata": {
        "id": "Ko1DAxupMVH2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# att = BahdanauAttention(128)"
      ],
      "metadata": {
        "id": "p3zHlFYaOBH2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query = torch.randn((32, 1 ,128))\n",
        "# keys = torch.randn((32, 10, 128))"
      ],
      "metadata": {
        "id": "AOPhdY6BOHN7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# context, weights = att.forward(query, keys)"
      ],
      "metadata": {
        "id": "6nQfa7NrOAN_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "    super(AttnDecoderRNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.attention = BahdanauAttention(hidden_size)\n",
        "    self.gru = nn.GRU(2*hidden_size, hidden_size, batch_first=True)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "  def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "    batch_size = encoder_outputs.size(0)\n",
        "    decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "    # print('initial_decoder_inp : ', decoder_input.shape)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    # print('decoder_hidden:', decoder_hidden.shape)\n",
        "    decoder_outputs = []\n",
        "    attentions = []\n",
        "    for i in range(MAX_LEN):\n",
        "      decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "          decoder_input, decoder_hidden, encoder_outputs\n",
        "      )\n",
        "      decoder_outputs.append(decoder_output)\n",
        "      attentions.append(attn_weights)\n",
        "\n",
        "      if target_tensor is not None:\n",
        "        decoder_input = target_tensor[:,i].unsqueeze(1)\n",
        "\n",
        "      else:\n",
        "        _, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "    decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "    decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "    attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "    return decoder_outputs, decoder_hidden, attentions\n",
        "  def forward_step(self, inp, hidden, encoder_outputs):\n",
        "    # print('inp : ', inp.shape)\n",
        "    embedded = self.dropout(self.embedding(inp))\n",
        "    # print('hidden : ', hidden.shape)\n",
        "    query = hidden.permute(1,0,2)\n",
        "    # print('query : ', query.shape)\n",
        "    # print('enc_out : ', encoder_outputs.shape)\n",
        "    context, attn_weights = self.attention(query, encoder_outputs)\n",
        "    input_gru = torch.cat((embedded, context), dim=2)\n",
        "    # print('input_gru :', input_gru.shape)\n",
        "    output, hidden  = self.gru(input_gru, hidden)\n",
        "    output = self.out(output)\n",
        "\n",
        "    return output, hidden, attn_weights\n"
      ],
      "metadata": {
        "id": "9KhlRxfTOWz5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# enc = EncoderRNN(input_lang.n_words, hidden_size=128)"
      ],
      "metadata": {
        "id": "cDqypEjfS6fN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rand_inp = torch.randint(1, input_lang.n_words, size=(32,10))"
      ],
      "metadata": {
        "id": "57RpDVHRS8qp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output, hidden = enc.forward(rand_inp)"
      ],
      "metadata": {
        "id": "YBxlqtypS-XL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dec = AttnDecoderRNN(128, output_lang.n_words)"
      ],
      "metadata": {
        "id": "luFKC0FPSpSc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ran_tar = torch.randint(1, output_lang.n_words, size=(32,10))"
      ],
      "metadata": {
        "id": "170o6zhTTHKJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# out, hid, att = dec.forward(output, hidden, ran_tar)"
      ],
      "metadata": {
        "id": "GBR6rZLRSwNJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "  indexes = indexesFromSentence(lang, sentence)\n",
        "  indexes.append(EOS_token)\n",
        "  return torch.tensor(indexes, dtype=torch.long, device=device).view(1,-1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "  return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "  input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "  n = len(pairs)\n",
        "  input_ids = np.zeros((n,MAX_LEN), dtype=np.int32)\n",
        "  target_ids = np.zeros((n,MAX_LEN), dtype=np.int32)\n",
        "\n",
        "  for idx, (inp, tgt) in enumerate(pairs):\n",
        "    inp_ids = indexesFromSentence(input_lang, inp)\n",
        "    tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "    inp_ids.append(EOS_token)\n",
        "    tgt_ids.append(EOS_token)\n",
        "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "    target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "  train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                             torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "  train_sampler = RandomSampler(train_data)\n",
        "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "  return input_lang, output_lang, train_dataloader\n",
        "\n"
      ],
      "metadata": {
        "id": "Uq78n3jDTExV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(data_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "\n",
        "  total_loss = 0\n",
        "  for data in data_loader:\n",
        "    input_tensor, target_tensor = data\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "    decoder_outputs,_,_ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "    loss = criterion(\n",
        "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "        target_tensor.view(-1)\n",
        "    )\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "  return total_loss / len(data_loader)\n",
        ""
      ],
      "metadata": {
        "id": "qzWJIenDqjxl"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "psKLxkoBrk7a"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "metadata": {
        "id": "SC83c9lvs9ls"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
        "  start = time.time()\n",
        "  plot_losses = []\n",
        "  print_loss_total = 0\n",
        "  plot_loss_total = 0\n",
        "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  for epoch in range(1, n_epochs + 1):\n",
        "    loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "    print_loss_total += loss\n",
        "    if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n"
      ],
      "metadata": {
        "id": "jK0ByxfUro09"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "\n",
        "    decoder_outputs, decoder_hidden, decoder_attention = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "    _, topi = decoder_outputs.topk(1)\n",
        "\n",
        "    decoded_ids = topi.squeeze()\n",
        "    decoded_words = []\n",
        "\n",
        "    for idx in decoded_ids:\n",
        "      if idx.item()==EOS_token:\n",
        "        decoded_words.append('<EOS>')\n",
        "        break\n",
        "      else:\n",
        "        decoded_words.append(output_lang.index2word[idx.item()])\n",
        "  return decoded_words, decoder_attention"
      ],
      "metadata": {
        "id": "KqIfxAqRtDD0"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "c-lGmv_MuMZ1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "\n",
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aGa-bUbuNCk",
        "outputId": "6c431873-0491-41f0-a062-9d7caf5eb3dd"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Lines......\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 105692 sentence pairs\n",
            "Counting words....\n",
            "Counted Words\n",
            "fra 17864\n",
            "eng 10698\n",
            "5m 38s (- 84m 41s) (5 6%) 1.3193\n",
            "11m 13s (- 78m 33s) (10 12%) 0.6265\n",
            "16m 47s (- 72m 45s) (15 18%) 0.4634\n",
            "22m 20s (- 67m 1s) (20 25%) 0.3828\n",
            "27m 55s (- 61m 25s) (25 31%) 0.3331\n",
            "33m 30s (- 55m 50s) (30 37%) 0.2985\n",
            "39m 8s (- 50m 19s) (35 43%) 0.2731\n",
            "44m 44s (- 44m 44s) (40 50%) 0.2536\n",
            "50m 22s (- 39m 10s) (45 56%) 0.2378\n",
            "55m 56s (- 33m 34s) (50 62%) 0.2253\n",
            "61m 31s (- 27m 57s) (55 68%) 0.2148\n",
            "67m 6s (- 22m 22s) (60 75%) 0.2058\n",
            "72m 39s (- 16m 46s) (65 81%) 0.1981\n",
            "78m 12s (- 11m 10s) (70 87%) 0.1918\n",
            "83m 45s (- 5m 35s) (75 93%) 0.1860\n",
            "89m 19s (- 0m 0s) (80 100%) 0.1808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, decoder, 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlAdbK42uTw7",
        "outputId": "bbfbe030-3ef3-4028-ed05-1f9357bdb407"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> ce fut extremement douloureux\n",
            "= it was excruciating\n",
            "< it was excruciating <EOS>\n",
            "\n",
            "> nous sommes impatients de vous voir\n",
            "= we are looking forward to seeing you soon\n",
            "< we can t wait to meet you d <EOS>\n",
            "\n",
            "> tout le monde fut surpris\n",
            "= everyone was surprised\n",
            "< everyone was surprised surprised <EOS>\n",
            "\n",
            "> vous n etes pas les seuls avec ce probleme\n",
            "= you re not the only one with this problem\n",
            "< you re not the only one with this problem <EOS>\n",
            "\n",
            "> ou es tu exactement ?\n",
            "= where exactly are you ?\n",
            "< where exactly did you get ? <EOS>\n",
            "\n",
            "> puis je partir maintenant ?\n",
            "= can i go now ?\n",
            "< can i go now ? <EOS>\n",
            "\n",
            "> nous ne sommes pas en forme\n",
            "= we re not fit\n",
            "< we re not fit to make <EOS>\n",
            "\n",
            "> he ouvrez la porte\n",
            "= hey open the door\n",
            "< hey open the door open the door <EOS>\n",
            "\n",
            "> votre fille n est plus une enfant\n",
            "= your daughter is not a child any more\n",
            "< your daughter is more like a child any more <EOS>\n",
            "\n",
            "> comment l as tu connu ?\n",
            "= how did you get to know him ?\n",
            "< how did you know him about it ? <EOS>\n",
            "\n",
            "> pourquoi ne l as tu pas ecoute ?\n",
            "= why didn t you listen to him ?\n",
            "< why didn t you listen to him ? <EOS>\n",
            "\n",
            "> est ce que tous les oiseaux peuvent voler ?\n",
            "= can all birds fly ?\n",
            "< can all birds fly ? <EOS>\n",
            "\n",
            "> vous etes libres de vous en aller\n",
            "= you re free to go\n",
            "< you are free to get to go <EOS>\n",
            "\n",
            "> a quelle sorte de jeu joues tu ?\n",
            "= what kind of game are you playing ?\n",
            "< what kind of game are you playing ? <EOS>\n",
            "\n",
            "> je ne sais pas ce que cela signifiait\n",
            "= i don t know what that meant\n",
            "< i don t know what that meant <EOS>\n",
            "\n",
            "> c est un tres chouette costume\n",
            "= that s a very nice suit\n",
            "< it s a very nice suit suit <EOS>\n",
            "\n",
            "> il est l heure de vous lever\n",
            "= it s time for you to get up\n",
            "< it s time to you getting up <EOS>\n",
            "\n",
            "> j ai deja une enveloppe\n",
            "= i already have an envelope\n",
            "< i already have an envelope yet <EOS>\n",
            "\n",
            "> j ai ete gifle sur les deux joues\n",
            "= i got slapped on both cheeks\n",
            "< i ve been trying to play two cheeks <EOS>\n",
            "\n",
            "> soyez desormais plus prudente\n",
            "= be more careful from now on\n",
            "< be more careful from now on more and more <EOS>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkrStO57FM8S"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}