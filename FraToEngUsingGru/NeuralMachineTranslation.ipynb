{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ecd738b-59d5-4e8e-a113-0fb1f9b82dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5b901c-2c71-4649-b6dc-9a6ab3176c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"data (1).zip\", 'r') as zObject:\n",
    "    zObject.extractall(path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6343aa32-9b04-4632-a18b-c45d28d385fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.word2index = {}\n",
    "    self.word2count = {}\n",
    "    self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "    self.n_words = 2\n",
    "  def addSentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.addWord(word)\n",
    "  def addWord(self, word):\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f75c47-be71-4cbf-a812-eed583461065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "  return ''.join(\n",
    "      c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn'\n",
    "  )\n",
    "\n",
    "def normalizeString(s):\n",
    "  s = unicodeToAscii(s.lower().strip())\n",
    "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "  s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "  return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472513d5-3af4-47eb-ab69-7cc0f883eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "  print(\"Reading Lines......\")\n",
    "\n",
    "  lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "  if reverse:\n",
    "    pairs = [list(reversed(p)) for p in pairs]\n",
    "    input_lang = Lang(lang2)\n",
    "    output_lang = Lang(lang1)\n",
    "  else:\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "  return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d1d021-6118-44ce-b447-5bcdc8abc0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "  return len(p[0].split(' ')) < MAX_LEN and len(p[1].split(' ')) < MAX_LEN and p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "  return [p for p in pairs if filterPair(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091d638c-7363-4b2f-85f0-e29e66f37f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines......\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words....\n",
      "Counted Words\n",
      "fra 4601\n",
      "eng 2991\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse = False):\n",
    "  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "  print(\"Read %s sentence pairs\" %len(pairs))\n",
    "  pairs = filterPairs(pairs)\n",
    "  print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "  print(\"Counting words....\")\n",
    "  for pair in pairs:\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "  print(\"Counted Words\")\n",
    "  print(input_lang.name, input_lang.n_words)\n",
    "  print(output_lang.name, output_lang.n_words)\n",
    "  return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng','fra',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b37c37-e984-4f4c-9688-f8208e2dbdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je ne suis pas un bon menteur', 'i m not a good liar']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5717ae85-8018-4485-88ab-2a4ec269c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, input):\n",
    "    embedded = self.dropout(self.embedding(input))\n",
    "    # print('embedded :', embedded.shape)\n",
    "    output, hidden = self.gru(embedded)\n",
    "    # print('output :', output.shape)\n",
    "    # print('Hidden :', hidden[0].shape)\n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "576432b8-8e71-49e5-b0fb-ed4559dd3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LEN):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10462944-9368-4c73-b6fa-7a80cbc2623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "  indexes = indexesFromSentence(lang, sentence)\n",
    "  indexes.append(EOS_token)\n",
    "  return torch.tensor(indexes, dtype=torch.long, device=device).view(1,-1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "  return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "  input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "  n = len(pairs)\n",
    "  input_ids = np.zeros((n,MAX_LEN), dtype=np.int32)\n",
    "  target_ids = np.zeros((n,MAX_LEN), dtype=np.int32)\n",
    "\n",
    "  for idx, (inp, tgt) in enumerate(pairs):\n",
    "    inp_ids = indexesFromSentence(input_lang, inp)\n",
    "    tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "    inp_ids.append(EOS_token)\n",
    "    tgt_ids.append(EOS_token)\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "  train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                             torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "  return input_lang, output_lang, train_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c64ffdd6-2efa-4c10-8866-4a0b1f8fa37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "  total_loss = 0\n",
    "  for data in data_loader:\n",
    "    input_tensor, target_tensor = data\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_outputs,_,_ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        target_tensor.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "  return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e32e189-bfda-485f-ba7d-a56d235dde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0589f425-0ab5-45e7-a4a3-0a42538e3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
    "  start = time.time()\n",
    "  plot_losses = []\n",
    "  print_loss_total = 0\n",
    "  plot_loss_total = 0\n",
    "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  criterion = nn.NLLLoss()\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print_loss_total += loss\n",
    "    if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b37214fe-61b1-4d29-b82d-cbe7ccb8bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "  with torch.no_grad():\n",
    "    input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "    decoder_outputs, decoder_hidden, decoder_attention = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "    decoded_words = []\n",
    "\n",
    "    for idx in decoded_ids:\n",
    "      if idx.item()==EOS_token:\n",
    "        decoded_words.append('<EOS>')\n",
    "        break\n",
    "      else:\n",
    "        decoded_words.append(output_lang.index2word[idx.item()])\n",
    "  return decoded_words, decoder_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0d52fa1-8985-4302-95a7-448886fd3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe81cc01-0b1d-46e8-8cc3-1b81df536fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines......\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words....\n",
      "Counted Words\n",
      "fra 4601\n",
      "eng 2991\n",
      "0m 32s (- 8m 2s) (5 6%) 1.6980\n",
      "1m 2s (- 7m 19s) (10 12%) 0.9260\n",
      "1m 31s (- 6m 37s) (15 18%) 0.6359\n",
      "2m 0s (- 6m 1s) (20 25%) 0.4639\n",
      "2m 32s (- 5m 34s) (25 31%) 0.3488\n",
      "3m 7s (- 5m 13s) (30 37%) 0.2698\n",
      "3m 42s (- 4m 45s) (35 43%) 0.2108\n",
      "4m 12s (- 4m 12s) (40 50%) 0.1690\n",
      "4m 44s (- 3m 41s) (45 56%) 0.1366\n",
      "5m 16s (- 3m 9s) (50 62%) 0.1136\n",
      "5m 47s (- 2m 37s) (55 68%) 0.0953\n",
      "6m 19s (- 2m 6s) (60 75%) 0.0828\n",
      "6m 51s (- 1m 34s) (65 81%) 0.0721\n",
      "7m 22s (- 1m 3s) (70 87%) 0.0644\n",
      "7m 55s (- 0m 31s) (75 93%) 0.0582\n",
      "8m 26s (- 0m 0s) (80 100%) 0.0543\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "176cdfb7-8d0c-42af-9786-68de2825f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> vous etes sur mon chemin\n",
      "= you are in my way\n",
      "< you are good at home <EOS>\n",
      "\n",
      "> nous sommes timides\n",
      "= we re shy\n",
      "< we re happy <EOS>\n",
      "\n",
      "> je prends mon apres midi demain\n",
      "= i m taking tomorrow afternoon off\n",
      "< i m taking tomorrow afternoon off <EOS>\n",
      "\n",
      "> je m inquiete pour l avenir\n",
      "= i m worried about the future\n",
      "< i m just trying for the office room <EOS>\n",
      "\n",
      "> tu n es pas le seul avec ce probleme\n",
      "= you re not the only one with this problem\n",
      "< you re not the only one with this problem <EOS>\n",
      "\n",
      "> il patine\n",
      "= he is skating\n",
      "< he is skating <EOS>\n",
      "\n",
      "> vous etes fort attirant\n",
      "= you re very attractive\n",
      "< you re very attractive in blue <EOS>\n",
      "\n",
      "> je suis tres sensible au froid\n",
      "= i am very sensitive to the cold\n",
      "< i am very sensitive to the cold weather <EOS>\n",
      "\n",
      "> elle a de l assurance\n",
      "= she s assertive\n",
      "< she s new here aren t you ? <EOS>\n",
      "\n",
      "> je suis impatient de vous rencontrer\n",
      "= i am looking forward to seeing you\n",
      "< i am looking forward to seeing you again <EOS>\n",
      "\n",
      "> c est un garcon intelligent\n",
      "= he is a clever boy\n",
      "< he is such a clever such ! <EOS>\n",
      "\n",
      "> je ne suis pas d humeur a blaguer\n",
      "= i am in no mood for joking\n",
      "< i am not in the mood right now <EOS>\n",
      "\n",
      "> je suis motivee\n",
      "= i m motivated\n",
      "< i m amazed with it <EOS>\n",
      "\n",
      "> nous allons bien maintenant\n",
      "= we re all right now\n",
      "< we re all just fine for that <EOS>\n",
      "\n",
      "> je ne suis pas desesperee\n",
      "= i m not desperate\n",
      "< i m not kind of that <EOS>\n",
      "\n",
      "> c est le portrait crache de son pere\n",
      "= he is the spitting image of his father\n",
      "< he is the spitting image of his father <EOS>\n",
      "\n",
      "> je suis desole d etre en retard\n",
      "= i m sorry for being late\n",
      "< i m sorry for not being more supportive <EOS>\n",
      "\n",
      "> je suis plus degourdi que toi\n",
      "= i m smarter than you\n",
      "< i m tougher you what i want <EOS>\n",
      "\n",
      "> je ne suis pas mechante\n",
      "= i m not mean\n",
      "< i m not quite drunk <EOS>\n",
      "\n",
      "> vous etes fort grossieres\n",
      "= you re very rude\n",
      "< you re very fit <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder, decoder, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b675a-5e56-49a1-824b-bb516c94eeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
