{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ecd738b-59d5-4e8e-a113-0fb1f9b82dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5b901c-2c71-4649-b6dc-9a6ab3176c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(\"data (1).zip\", 'r') as zObject:\n",
    "    zObject.extractall(path=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6343aa32-9b04-4632-a18b-c45d28d385fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "  def __init__(self, name):\n",
    "    self.name = name\n",
    "    self.word2index = {}\n",
    "    self.word2count = {}\n",
    "    self.index2word = {0:\"SOS\", 1:\"EOS\"}\n",
    "    self.n_words = 2\n",
    "  def addSentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.addWord(word)\n",
    "  def addWord(self, word):\n",
    "    if word not in self.word2index:\n",
    "      self.word2index[word] = self.n_words\n",
    "      self.word2count[word] = 1\n",
    "      self.index2word[self.n_words] = word\n",
    "      self.n_words += 1\n",
    "    else:\n",
    "      self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f75c47-be71-4cbf-a812-eed583461065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "  return ''.join(\n",
    "      c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn'\n",
    "  )\n",
    "\n",
    "def normalizeString(s):\n",
    "  s = unicodeToAscii(s.lower().strip())\n",
    "  s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "  s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "  return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472513d5-3af4-47eb-ab69-7cc0f883eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "  print(\"Reading Lines......\")\n",
    "\n",
    "  lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').read().strip().split('\\n')\n",
    "\n",
    "  pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "  if reverse:\n",
    "    pairs = [list(reversed(p)) for p in pairs]\n",
    "    input_lang = Lang(lang2)\n",
    "    output_lang = Lang(lang1)\n",
    "  else:\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "\n",
    "  return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d1d021-6118-44ce-b447-5bcdc8abc0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "  return len(p[0].split(' ')) < MAX_LEN and len(p[1].split(' ')) < MAX_LEN and p[1].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "  return [p for p in pairs if filterPair(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "091d638c-7363-4b2f-85f0-e29e66f37f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines......\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words....\n",
      "Counted Words\n",
      "fra 4601\n",
      "eng 2991\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse = False):\n",
    "  input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "  print(\"Read %s sentence pairs\" %len(pairs))\n",
    "  pairs = filterPairs(pairs)\n",
    "  print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "  print(\"Counting words....\")\n",
    "  for pair in pairs:\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "  print(\"Counted Words\")\n",
    "  print(input_lang.name, input_lang.n_words)\n",
    "  print(output_lang.name, output_lang.n_words)\n",
    "  return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng','fra',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52b37c37-e984-4f4c-9688-f8208e2dbdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je ne suis pas un bon menteur', 'i m not a good liar']\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5717ae85-8018-4485-88ab-2a4ec269c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "    self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "    self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "  def forward(self, input):\n",
    "    embedded = self.dropout(self.embedding(input))\n",
    "    # print('embedded :', embedded.shape)\n",
    "    output, hidden = self.gru(embedded)\n",
    "    # print('output :', output.shape)\n",
    "    # print('Hidden :', hidden[0].shape)\n",
    "    return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "576432b8-8e71-49e5-b0fb-ed4559dd3f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LEN):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10462944-9368-4c73-b6fa-7a80cbc2623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "  indexes = indexesFromSentence(lang, sentence)\n",
    "  indexes.append(EOS_token)\n",
    "  return torch.tensor(indexes, dtype=torch.long, device=device).view(1,-1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "  return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "  input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
    "\n",
    "  n = len(pairs)\n",
    "  input_ids = np.zeros((n,MAX_LEN), dtype=np.int32)\n",
    "  target_ids = np.zeros((n,MAX_LEN), dtype=np.int32)\n",
    "\n",
    "  for idx, (inp, tgt) in enumerate(pairs):\n",
    "    inp_ids = indexesFromSentence(input_lang, inp)\n",
    "    tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "    inp_ids.append(EOS_token)\n",
    "    tgt_ids.append(EOS_token)\n",
    "    input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "    target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "  train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                             torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "  train_sampler = RandomSampler(train_data)\n",
    "  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "  return input_lang, output_lang, train_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c64ffdd6-2efa-4c10-8866-4a0b1f8fa37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "\n",
    "  total_loss = 0\n",
    "  for data in data_loader:\n",
    "    input_tensor, target_tensor = data\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_outputs,_,_ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "    loss = criterion(\n",
    "        decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "        target_tensor.view(-1)\n",
    "    )\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "  return total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e32e189-bfda-485f-ba7d-a56d235dde55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0589f425-0ab5-45e7-a4a3-0a42538e3b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
    "  start = time.time()\n",
    "  plot_losses = []\n",
    "  print_loss_total = 0\n",
    "  plot_loss_total = 0\n",
    "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  criterion = nn.NLLLoss()\n",
    "\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print_loss_total += loss\n",
    "    if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b37214fe-61b1-4d29-b82d-cbe7ccb8bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "  with torch.no_grad():\n",
    "    input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "    decoder_outputs, decoder_hidden, decoder_attention = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "    _, topi = decoder_outputs.topk(1)\n",
    "    decoded_ids = topi.squeeze()\n",
    "    decoded_words = []\n",
    "\n",
    "    for idx in decoded_ids:\n",
    "      if idx.item()==EOS_token:\n",
    "        decoded_words.append('<EOS>')\n",
    "        break\n",
    "      else:\n",
    "        decoded_words.append(output_lang.index2word[idx.item()])\n",
    "  return decoded_words, decoder_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0d52fa1-8985-4302-95a7-448886fd3a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe81cc01-0b1d-46e8-8cc3-1b81df536fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Lines......\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 11445 sentence pairs\n",
      "Counting words....\n",
      "Counted Words\n",
      "fra 4601\n",
      "eng 2991\n",
      "0m 32s (- 8m 2s) (5 6%) 1.6980\n",
      "1m 2s (- 7m 19s) (10 12%) 0.9260\n",
      "1m 31s (- 6m 37s) (15 18%) 0.6359\n",
      "2m 0s (- 6m 1s) (20 25%) 0.4639\n",
      "2m 32s (- 5m 34s) (25 31%) 0.3488\n",
      "3m 7s (- 5m 13s) (30 37%) 0.2698\n",
      "3m 42s (- 4m 45s) (35 43%) 0.2108\n",
      "4m 12s (- 4m 12s) (40 50%) 0.1690\n",
      "4m 44s (- 3m 41s) (45 56%) 0.1366\n",
      "5m 16s (- 3m 9s) (50 62%) 0.1136\n",
      "5m 47s (- 2m 37s) (55 68%) 0.0953\n",
      "6m 19s (- 2m 6s) (60 75%) 0.0828\n",
      "6m 51s (- 1m 34s) (65 81%) 0.0721\n",
      "7m 22s (- 1m 3s) (70 87%) 0.0644\n",
      "7m 55s (- 0m 31s) (75 93%) 0.0582\n",
      "8m 26s (- 0m 0s) (80 100%) 0.0543\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "176cdfb7-8d0c-42af-9786-68de2825f1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> je suis flic\n",
      "= i m a cop\n",
      "< i m a cop <EOS>\n",
      "\n",
      "> heureuse de te voir en un seul morceau !\n",
      "= i m glad to see you in one piece\n",
      "< i m happy to see you in one piece <EOS>\n",
      "\n",
      "> je sais skier\n",
      "= i m able to ski\n",
      "< i m able to speak french <EOS>\n",
      "\n",
      "> il n est pas tres bon en mathematiques\n",
      "= he is not very good at mathematics\n",
      "< he is not very good at mathematics <EOS>\n",
      "\n",
      "> je suis sure de remporter le match de tennis\n",
      "= i m sure i ll win the tennis match\n",
      "< i m sure i ll win the tennis match <EOS>\n",
      "\n",
      "> je ne suis pas en train de manger\n",
      "= i am not eating\n",
      "< i am not eating <EOS>\n",
      "\n",
      "> elle lui est apparentee de maniere lointaine\n",
      "= she is distantly related to him\n",
      "< she is distantly related to him by nature <EOS>\n",
      "\n",
      "> je suis bon\n",
      "= i am good\n",
      "< i am good at math <EOS>\n",
      "\n",
      "> nous n y allons pas si ?\n",
      "= we re not going are we ?\n",
      "< we re not going are we ? <EOS>\n",
      "\n",
      "> je suis differente\n",
      "= i m different\n",
      "< i m different <EOS>\n",
      "\n",
      "> j ai peur des ours\n",
      "= i am afraid of bears\n",
      "< i am afraid of earthquakes <EOS>\n",
      "\n",
      "> je ne suis pas amer\n",
      "= i m not bitter\n",
      "< i m not one of this <EOS>\n",
      "\n",
      "> vous etes tellement gentilles !\n",
      "= you re so sweet\n",
      "< you re so sweet aren t you ? <EOS>\n",
      "\n",
      "> je suis encore endormie\n",
      "= i m still sleepy\n",
      "< i m still not with on schedule <EOS>\n",
      "\n",
      "> vous etes la de bonne heure ce matin !\n",
      "= you are very early this morning\n",
      "< you are very early this morning <EOS>\n",
      "\n",
      "> je suis tranquille\n",
      "= i m quiet\n",
      "< i m quiet <EOS>\n",
      "\n",
      "> il est confronte a de nombreuses difficultes\n",
      "= he is confronted by many difficulties\n",
      "< he is absorbed in the study ready <EOS>\n",
      "\n",
      "> c est une autorite en matiere d humanites\n",
      "= he is an authority on the humanities\n",
      "< he is an authority on the humanities <EOS>\n",
      "\n",
      "> nous n y allons pas\n",
      "= we re not going\n",
      "< we re not going to fight <EOS>\n",
      "\n",
      "> nous n allons pas le jeter\n",
      "= we re not throwing it away\n",
      "< we re not throwing it away want <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder, decoder, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a25b675a-5e56-49a1-824b-bb516c94eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.28764198060873264\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [word_tokenize(sentence.lower()) for sentence in reference]\n",
    "    candidate = word_tokenize(candidate.lower())\n",
    "\n",
    "    smoothing_function = SmoothingFunction().method1  # You can choose different smoothing methods\n",
    "\n",
    "    return sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
    "\n",
    "# Example usage:\n",
    "reference_sentence = [\"The cat is sitting on the mat\"]\n",
    "candidate_sentence = \"The cat is on the mat\"\n",
    "\n",
    "bleu_score = calculate_bleu(reference_sentence, candidate_sentence)\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc72d22f-88b0-4c6a-8db7-5581f6c1cef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def bleu_metric(encoder, decoder, n=10):\n",
    "    bleu=0\n",
    "    for i in tqdm(range(n)):\n",
    "        pair = random.choice(pairs)\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        reference_sentence = [pair[1]]\n",
    "        candidate_sentence = output_sentence\n",
    "        bleu+= calculate_bleu(reference_sentence, candidate_sentence)\n",
    "    return bleu/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b19624c4-e8c2-43c9-b308-1c12de55d6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:11<00:00, 905.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28670634570042225"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric(encoder, decoder, n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b7a96-d754-479c-811f-07c509cfc55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
